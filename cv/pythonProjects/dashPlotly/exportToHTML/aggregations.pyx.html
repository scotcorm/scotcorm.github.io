<html>
<head>
<title>aggregations.pyx</title>
<meta http-equiv="Content-Type" content="text/html; charset=utf-8">
<style type="text/css">
.s0 { color: #a9b7c6;}
</style>
</head>
<body bgcolor="#2b2b2b">
<table CELLSPACING=0 CELLPADDING=5 COLS=1 WIDTH="100%" BGCOLOR="#606060" >
<tr><td><center>
<font face="Arial, Helvetica" color="#000000">
aggregations.pyx</font>
</center></td></tr></table>
<pre><span class="s0"># cython: boundscheck=False, wraparound=False, cdivision=True</span>

<span class="s0">import cython</span>

<span class="s0">from libc.math cimport round</span>
<span class="s0">from libcpp.deque cimport deque</span>

<span class="s0">from pandas._libs.algos cimport TiebreakEnumType</span>

<span class="s0">import numpy as np</span>

<span class="s0">cimport numpy as cnp</span>
<span class="s0">from numpy cimport (</span>
    <span class="s0">float32_t,</span>
    <span class="s0">float64_t,</span>
    <span class="s0">int64_t,</span>
    <span class="s0">ndarray,</span>
<span class="s0">)</span>

<span class="s0">cnp.import_array()</span>


<span class="s0">cdef extern from &quot;../src/headers/cmath&quot; namespace &quot;std&quot;:</span>
    <span class="s0">bint isnan(float64_t) nogil</span>
    <span class="s0">bint notnan(float64_t) nogil</span>
    <span class="s0">int signbit(float64_t) nogil</span>
    <span class="s0">float64_t sqrt(float64_t x) nogil</span>

<span class="s0">from pandas._libs.algos import is_monotonic</span>
<span class="s0">from pandas._libs.dtypes cimport numeric_t</span>


<span class="s0">cdef extern from &quot;../src/skiplist.h&quot;:</span>
    <span class="s0">ctypedef struct node_t:</span>
        <span class="s0">node_t **next</span>
        <span class="s0">int *width</span>
        <span class="s0">double value</span>
        <span class="s0">int is_nil</span>
        <span class="s0">int levels</span>
        <span class="s0">int ref_count</span>

    <span class="s0">ctypedef struct skiplist_t:</span>
        <span class="s0">node_t *head</span>
        <span class="s0">node_t **tmp_chain</span>
        <span class="s0">int *tmp_steps</span>
        <span class="s0">int size</span>
        <span class="s0">int maxlevels</span>

    <span class="s0">skiplist_t* skiplist_init(int) nogil</span>
    <span class="s0">void skiplist_destroy(skiplist_t*) nogil</span>
    <span class="s0">double skiplist_get(skiplist_t*, int, int*) nogil</span>
    <span class="s0">int skiplist_insert(skiplist_t*, double) nogil</span>
    <span class="s0">int skiplist_remove(skiplist_t*, double) nogil</span>
    <span class="s0">int skiplist_rank(skiplist_t*, double) nogil</span>
    <span class="s0">int skiplist_min_rank(skiplist_t*, double) nogil</span>

<span class="s0">cdef:</span>
    <span class="s0">float32_t MINfloat32 = np.NINF</span>
    <span class="s0">float64_t MINfloat64 = np.NINF</span>

    <span class="s0">float32_t MAXfloat32 = np.inf</span>
    <span class="s0">float64_t MAXfloat64 = np.inf</span>

    <span class="s0">float64_t NaN = &lt;float64_t&gt;np.NaN</span>

<span class="s0">cdef bint is_monotonic_increasing_start_end_bounds(</span>
    <span class="s0">ndarray[int64_t, ndim=1] start, ndarray[int64_t, ndim=1] end</span>
<span class="s0">):</span>
    <span class="s0">return is_monotonic(start, False)[0] and is_monotonic(end, False)[0]</span>

<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling sum</span>


<span class="s0">cdef inline float64_t calc_sum(int64_t minp, int64_t nobs, float64_t sum_x) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t result</span>

    <span class="s0">if nobs == 0 == minp:</span>
        <span class="s0">result = 0</span>
    <span class="s0">elif nobs &gt;= minp:</span>
        <span class="s0">result = sum_x</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>

    <span class="s0">return result</span>


<span class="s0">cdef inline void add_sum(float64_t val, int64_t *nobs, float64_t *sum_x,</span>
                         <span class="s0">float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; add a value from the sum calc using Kahan summation &quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] + 1</span>
        <span class="s0">y = val - compensation[0]</span>
        <span class="s0">t = sum_x[0] + y</span>
        <span class="s0">compensation[0] = t - sum_x[0] - y</span>
        <span class="s0">sum_x[0] = t</span>


<span class="s0">cdef inline void remove_sum(float64_t val, int64_t *nobs, float64_t *sum_x,</span>
                            <span class="s0">float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the sum calc using Kahan summation &quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>
        <span class="s0">y = - val - compensation[0]</span>
        <span class="s0">t = sum_x[0] + y</span>
        <span class="s0">compensation[0] = t - sum_x[0] - y</span>
        <span class="s0">sum_x[0] = t</span>


<span class="s0">def roll_sum(const float64_t[:] values, ndarray[int64_t] start,</span>
             <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j</span>
        <span class="s0">float64_t sum_x = 0, compensation_add = 0, compensation_remove = 0</span>
        <span class="s0">int64_t s, e</span>
        <span class="s0">int64_t nobs = 0, N = len(values)</span>
        <span class="s0">ndarray[float64_t] output</span>
        <span class="s0">bint is_monotonic_increasing_bounds</span>

    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">with nogil:</span>

        <span class="s0">for i in range(0, N):</span>
            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0"># setup</span>

                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">add_sum(values[j], &amp;nobs, &amp;sum_x, &amp;compensation_add)</span>

            <span class="s0">else:</span>

                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">remove_sum(values[j], &amp;nobs, &amp;sum_x, &amp;compensation_remove)</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">add_sum(values[j], &amp;nobs, &amp;sum_x, &amp;compensation_add)</span>

            <span class="s0">output[i] = calc_sum(minp, nobs, sum_x)</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0</span>
                <span class="s0">sum_x = 0.0</span>
                <span class="s0">compensation_remove = 0.0</span>

    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling mean</span>


<span class="s0">cdef inline float64_t calc_mean(int64_t minp, Py_ssize_t nobs,</span>
                                <span class="s0">Py_ssize_t neg_ct, float64_t sum_x) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t result</span>

    <span class="s0">if nobs &gt;= minp and nobs &gt; 0:</span>
        <span class="s0">result = sum_x / &lt;float64_t&gt;nobs</span>
        <span class="s0">if neg_ct == 0 and result &lt; 0:</span>
            <span class="s0"># all positive</span>
            <span class="s0">result = 0</span>
        <span class="s0">elif neg_ct == nobs and result &gt; 0:</span>
            <span class="s0"># all negative</span>
            <span class="s0">result = 0</span>
        <span class="s0">else:</span>
            <span class="s0">pass</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>
    <span class="s0">return result</span>


<span class="s0">cdef inline void add_mean(float64_t val, Py_ssize_t *nobs, float64_t *sum_x,</span>
                          <span class="s0">Py_ssize_t *neg_ct, float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; add a value from the mean calc using Kahan summation &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] + 1</span>
        <span class="s0">y = val - compensation[0]</span>
        <span class="s0">t = sum_x[0] + y</span>
        <span class="s0">compensation[0] = t - sum_x[0] - y</span>
        <span class="s0">sum_x[0] = t</span>
        <span class="s0">if signbit(val):</span>
            <span class="s0">neg_ct[0] = neg_ct[0] + 1</span>


<span class="s0">cdef inline void remove_mean(float64_t val, Py_ssize_t *nobs, float64_t *sum_x,</span>
                             <span class="s0">Py_ssize_t *neg_ct, float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the mean calc using Kahan summation &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>
        <span class="s0">y = - val - compensation[0]</span>
        <span class="s0">t = sum_x[0] + y</span>
        <span class="s0">compensation[0] = t - sum_x[0] - y</span>
        <span class="s0">sum_x[0] = t</span>
        <span class="s0">if signbit(val):</span>
            <span class="s0">neg_ct[0] = neg_ct[0] - 1</span>


<span class="s0">def roll_mean(const float64_t[:] values, ndarray[int64_t] start,</span>
              <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t val, compensation_add = 0, compensation_remove = 0, sum_x = 0</span>
        <span class="s0">int64_t s, e</span>
        <span class="s0">Py_ssize_t nobs = 0, i, j, neg_ct = 0, N = len(values)</span>
        <span class="s0">ndarray[float64_t] output</span>
        <span class="s0">bint is_monotonic_increasing_bounds</span>

    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">with nogil:</span>

        <span class="s0">for i in range(0, N):</span>
            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0"># setup</span>
                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">add_mean(val, &amp;nobs, &amp;sum_x, &amp;neg_ct, &amp;compensation_add)</span>

            <span class="s0">else:</span>

                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">remove_mean(val, &amp;nobs, &amp;sum_x, &amp;neg_ct, &amp;compensation_remove)</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">add_mean(val, &amp;nobs, &amp;sum_x, &amp;neg_ct, &amp;compensation_add)</span>

            <span class="s0">output[i] = calc_mean(minp, nobs, neg_ct, sum_x)</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0</span>
                <span class="s0">neg_ct = 0</span>
                <span class="s0">sum_x = 0.0</span>
                <span class="s0">compensation_remove = 0.0</span>
    <span class="s0">return output</span>

<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling variance</span>


<span class="s0">cdef inline float64_t calc_var(int64_t minp, int ddof, float64_t nobs,</span>
                               <span class="s0">float64_t ssqdm_x) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t result</span>

    <span class="s0"># Variance is unchanged if no observation is added or removed</span>
    <span class="s0">if (nobs &gt;= minp) and (nobs &gt; ddof):</span>

        <span class="s0"># pathological case</span>
        <span class="s0">if nobs == 1:</span>
            <span class="s0">result = 0</span>
        <span class="s0">else:</span>
            <span class="s0">result = ssqdm_x / (nobs - &lt;float64_t&gt;ddof)</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>

    <span class="s0">return result</span>


<span class="s0">cdef inline void add_var(float64_t val, float64_t *nobs, float64_t *mean_x,</span>
                         <span class="s0">float64_t *ssqdm_x, float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; add a value from the var calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t delta, prev_mean, y, t</span>

    <span class="s0"># `isnan` instead of equality as fix for GH-21813, msvc 2017 bug</span>
    <span class="s0">if isnan(val):</span>
        <span class="s0">return</span>

    <span class="s0">nobs[0] = nobs[0] + 1</span>
    <span class="s0"># Welford's method for the online variance-calculation</span>
    <span class="s0"># using Kahan summation</span>
    <span class="s0"># https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance</span>
    <span class="s0">prev_mean = mean_x[0] - compensation[0]</span>
    <span class="s0">y = val - compensation[0]</span>
    <span class="s0">t = y - mean_x[0]</span>
    <span class="s0">compensation[0] = t + mean_x[0] - y</span>
    <span class="s0">delta = t</span>
    <span class="s0">if nobs[0]:</span>
        <span class="s0">mean_x[0] = mean_x[0] + delta / nobs[0]</span>
    <span class="s0">else:</span>
        <span class="s0">mean_x[0] = 0</span>
    <span class="s0">ssqdm_x[0] = ssqdm_x[0] + (val - prev_mean) * (val - mean_x[0])</span>


<span class="s0">cdef inline void remove_var(float64_t val, float64_t *nobs, float64_t *mean_x,</span>
                            <span class="s0">float64_t *ssqdm_x, float64_t *compensation) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the var calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t delta, prev_mean, y, t</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>
        <span class="s0">if nobs[0]:</span>
            <span class="s0"># Welford's method for the online variance-calculation</span>
            <span class="s0"># using Kahan summation</span>
            <span class="s0"># https://en.wikipedia.org/wiki/Algorithms_for_calculating_variance</span>
            <span class="s0">prev_mean = mean_x[0] - compensation[0]</span>
            <span class="s0">y = val - compensation[0]</span>
            <span class="s0">t = y - mean_x[0]</span>
            <span class="s0">compensation[0] = t + mean_x[0] - y</span>
            <span class="s0">delta = t</span>
            <span class="s0">mean_x[0] = mean_x[0] - delta / nobs[0]</span>
            <span class="s0">ssqdm_x[0] = ssqdm_x[0] - (val - prev_mean) * (val - mean_x[0])</span>
        <span class="s0">else:</span>
            <span class="s0">mean_x[0] = 0</span>
            <span class="s0">ssqdm_x[0] = 0</span>


<span class="s0">def roll_var(const float64_t[:] values, ndarray[int64_t] start,</span>
             <span class="s0">ndarray[int64_t] end, int64_t minp, int ddof=1) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Numerically stable implementation using Welford's method.</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t mean_x = 0, ssqdm_x = 0, nobs = 0, compensation_add = 0,</span>
        <span class="s0">float64_t compensation_remove = 0,</span>
        <span class="s0">float64_t val, prev, delta, mean_x_old</span>
        <span class="s0">int64_t s, e</span>
        <span class="s0">Py_ssize_t i, j, N = len(values)</span>
        <span class="s0">ndarray[float64_t] output</span>
        <span class="s0">bint is_monotonic_increasing_bounds</span>

    <span class="s0">minp = max(minp, 1)</span>
    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">with nogil:</span>

        <span class="s0">for i in range(0, N):</span>

            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0"># Over the first window, observations can only be added</span>
            <span class="s0"># never removed</span>
            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">add_var(values[j], &amp;nobs, &amp;mean_x, &amp;ssqdm_x, &amp;compensation_add)</span>

            <span class="s0">else:</span>

                <span class="s0"># After the first window, observations can both be added</span>
                <span class="s0"># and removed</span>

                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">remove_var(values[j], &amp;nobs, &amp;mean_x, &amp;ssqdm_x,</span>
                               <span class="s0">&amp;compensation_remove)</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">add_var(values[j], &amp;nobs, &amp;mean_x, &amp;ssqdm_x, &amp;compensation_add)</span>

            <span class="s0">output[i] = calc_var(minp, ddof, nobs, ssqdm_x)</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0.0</span>
                <span class="s0">mean_x = 0.0</span>
                <span class="s0">ssqdm_x = 0.0</span>
                <span class="s0">compensation_remove = 0.0</span>

    <span class="s0">return output</span>

<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling skewness</span>


<span class="s0">cdef inline float64_t calc_skew(int64_t minp, int64_t nobs,</span>
                                <span class="s0">float64_t x, float64_t xx,</span>
                                <span class="s0">float64_t xxx) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t result, dnobs</span>
        <span class="s0">float64_t A, B, C, R</span>

    <span class="s0">if nobs &gt;= minp:</span>
        <span class="s0">dnobs = &lt;float64_t&gt;nobs</span>
        <span class="s0">A = x / dnobs</span>
        <span class="s0">B = xx / dnobs - A * A</span>
        <span class="s0">C = xxx / dnobs - A * A * A - 3 * A * B</span>

        <span class="s0"># #18044: with uniform distribution, floating issue will</span>
        <span class="s0">#         cause B != 0. and cause the result is a very</span>
        <span class="s0">#         large number.</span>
        <span class="s0">#</span>
        <span class="s0">#         in core/nanops.py nanskew/nankurt call the function</span>
        <span class="s0">#         _zero_out_fperr(m2) to fix floating error.</span>
        <span class="s0">#         if the variance is less than 1e-14, it could be</span>
        <span class="s0">#         treat as zero, here we follow the original</span>
        <span class="s0">#         skew/kurt behaviour to check B &lt;= 1e-14</span>
        <span class="s0">if B &lt;= 1e-14 or nobs &lt; 3:</span>
            <span class="s0">result = NaN</span>
        <span class="s0">else:</span>
            <span class="s0">R = sqrt(B)</span>
            <span class="s0">result = ((sqrt(dnobs * (dnobs - 1.)) * C) /</span>
                      <span class="s0">((dnobs - 2) * R * R * R))</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>

    <span class="s0">return result</span>


<span class="s0">cdef inline void add_skew(float64_t val, int64_t *nobs,</span>
                          <span class="s0">float64_t *x, float64_t *xx,</span>
                          <span class="s0">float64_t *xxx,</span>
                          <span class="s0">float64_t *compensation_x,</span>
                          <span class="s0">float64_t *compensation_xx,</span>
                          <span class="s0">float64_t *compensation_xxx) nogil:</span>
    <span class="s0">&quot;&quot;&quot; add a value from the skew calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] + 1</span>

        <span class="s0">y = val - compensation_x[0]</span>
        <span class="s0">t = x[0] + y</span>
        <span class="s0">compensation_x[0] = t - x[0] - y</span>
        <span class="s0">x[0] = t</span>
        <span class="s0">y = val * val - compensation_xx[0]</span>
        <span class="s0">t = xx[0] + y</span>
        <span class="s0">compensation_xx[0] = t - xx[0] - y</span>
        <span class="s0">xx[0] = t</span>
        <span class="s0">y = val * val * val - compensation_xxx[0]</span>
        <span class="s0">t = xxx[0] + y</span>
        <span class="s0">compensation_xxx[0] = t - xxx[0] - y</span>
        <span class="s0">xxx[0] = t</span>


<span class="s0">cdef inline void remove_skew(float64_t val, int64_t *nobs,</span>
                             <span class="s0">float64_t *x, float64_t *xx,</span>
                             <span class="s0">float64_t *xxx,</span>
                             <span class="s0">float64_t *compensation_x,</span>
                             <span class="s0">float64_t *compensation_xx,</span>
                             <span class="s0">float64_t *compensation_xxx) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the skew calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>

        <span class="s0">y = - val - compensation_x[0]</span>
        <span class="s0">t = x[0] + y</span>
        <span class="s0">compensation_x[0] = t - x[0] - y</span>
        <span class="s0">x[0] = t</span>
        <span class="s0">y = - val * val - compensation_xx[0]</span>
        <span class="s0">t = xx[0] + y</span>
        <span class="s0">compensation_xx[0] = t - xx[0] - y</span>
        <span class="s0">xx[0] = t</span>
        <span class="s0">y = - val * val * val - compensation_xxx[0]</span>
        <span class="s0">t = xxx[0] + y</span>
        <span class="s0">compensation_xxx[0] = t - xxx[0] - y</span>
        <span class="s0">xxx[0] = t</span>


<span class="s0">def roll_skew(ndarray[float64_t] values, ndarray[int64_t] start,</span>
              <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j</span>
        <span class="s0">float64_t val, prev, min_val, mean_val, sum_val = 0</span>
        <span class="s0">float64_t compensation_xxx_add = 0, compensation_xxx_remove = 0</span>
        <span class="s0">float64_t compensation_xx_add = 0, compensation_xx_remove = 0</span>
        <span class="s0">float64_t compensation_x_add = 0, compensation_x_remove = 0</span>
        <span class="s0">float64_t x = 0, xx = 0, xxx = 0</span>
        <span class="s0">int64_t nobs = 0, N = len(values), nobs_mean = 0</span>
        <span class="s0">int64_t s, e</span>
        <span class="s0">ndarray[float64_t] output, mean_array, values_copy</span>
        <span class="s0">bint is_monotonic_increasing_bounds</span>

    <span class="s0">minp = max(minp, 3)</span>
    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>
    <span class="s0">min_val = np.nanmin(values)</span>
    <span class="s0">values_copy = np.copy(values)</span>

    <span class="s0">with nogil:</span>
        <span class="s0">for i in range(0, N):</span>
            <span class="s0">val = values_copy[i]</span>
            <span class="s0">if notnan(val):</span>
                <span class="s0">nobs_mean += 1</span>
                <span class="s0">sum_val += val</span>
        <span class="s0">mean_val = sum_val / nobs_mean</span>
        <span class="s0"># Other cases would lead to imprecision for smallest values</span>
        <span class="s0">if min_val - mean_val &gt; -1e5:</span>
            <span class="s0">mean_val = round(mean_val)</span>
            <span class="s0">for i in range(0, N):</span>
                <span class="s0">values_copy[i] = values_copy[i] - mean_val</span>

        <span class="s0">for i in range(0, N):</span>

            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0"># Over the first window, observations can only be added</span>
            <span class="s0"># never removed</span>
            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">val = values_copy[j]</span>
                    <span class="s0">add_skew(val, &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;compensation_x_add,</span>
                             <span class="s0">&amp;compensation_xx_add, &amp;compensation_xxx_add)</span>

            <span class="s0">else:</span>

                <span class="s0"># After the first window, observations can both be added</span>
                <span class="s0"># and removed</span>
                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">val = values_copy[j]</span>
                    <span class="s0">remove_skew(val, &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;compensation_x_remove,</span>
                                <span class="s0">&amp;compensation_xx_remove, &amp;compensation_xxx_remove)</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">val = values_copy[j]</span>
                    <span class="s0">add_skew(val, &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;compensation_x_add,</span>
                             <span class="s0">&amp;compensation_xx_add, &amp;compensation_xxx_add)</span>

            <span class="s0">output[i] = calc_skew(minp, nobs, x, xx, xxx)</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0</span>
                <span class="s0">x = 0.0</span>
                <span class="s0">xx = 0.0</span>
                <span class="s0">xxx = 0.0</span>

    <span class="s0">return output</span>

<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling kurtosis</span>


<span class="s0">cdef inline float64_t calc_kurt(int64_t minp, int64_t nobs,</span>
                                <span class="s0">float64_t x, float64_t xx,</span>
                                <span class="s0">float64_t xxx, float64_t xxxx) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t result, dnobs</span>
        <span class="s0">float64_t A, B, C, D, R, K</span>

    <span class="s0">if nobs &gt;= minp:</span>
        <span class="s0">dnobs = &lt;float64_t&gt;nobs</span>
        <span class="s0">A = x / dnobs</span>
        <span class="s0">R = A * A</span>
        <span class="s0">B = xx / dnobs - R</span>
        <span class="s0">R = R * A</span>
        <span class="s0">C = xxx / dnobs - R - 3 * A * B</span>
        <span class="s0">R = R * A</span>
        <span class="s0">D = xxxx / dnobs - R - 6 * B * A * A - 4 * C * A</span>

        <span class="s0"># #18044: with uniform distribution, floating issue will</span>
        <span class="s0">#         cause B != 0. and cause the result is a very</span>
        <span class="s0">#         large number.</span>
        <span class="s0">#</span>
        <span class="s0">#         in core/nanops.py nanskew/nankurt call the function</span>
        <span class="s0">#         _zero_out_fperr(m2) to fix floating error.</span>
        <span class="s0">#         if the variance is less than 1e-14, it could be</span>
        <span class="s0">#         treat as zero, here we follow the original</span>
        <span class="s0">#         skew/kurt behaviour to check B &lt;= 1e-14</span>
        <span class="s0">if B &lt;= 1e-14 or nobs &lt; 4:</span>
            <span class="s0">result = NaN</span>
        <span class="s0">else:</span>
            <span class="s0">K = (dnobs * dnobs - 1.) * D / (B * B) - 3 * ((dnobs - 1.) ** 2)</span>
            <span class="s0">result = K / ((dnobs - 2.) * (dnobs - 3.))</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>

    <span class="s0">return result</span>


<span class="s0">cdef inline void add_kurt(float64_t val, int64_t *nobs,</span>
                          <span class="s0">float64_t *x, float64_t *xx,</span>
                          <span class="s0">float64_t *xxx, float64_t *xxxx,</span>
                          <span class="s0">float64_t *compensation_x,</span>
                          <span class="s0">float64_t *compensation_xx,</span>
                          <span class="s0">float64_t *compensation_xxx,</span>
                          <span class="s0">float64_t *compensation_xxxx) nogil:</span>
    <span class="s0">&quot;&quot;&quot; add a value from the kurotic calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] + 1</span>

        <span class="s0">y = val - compensation_x[0]</span>
        <span class="s0">t = x[0] + y</span>
        <span class="s0">compensation_x[0] = t - x[0] - y</span>
        <span class="s0">x[0] = t</span>
        <span class="s0">y = val * val - compensation_xx[0]</span>
        <span class="s0">t = xx[0] + y</span>
        <span class="s0">compensation_xx[0] = t - xx[0] - y</span>
        <span class="s0">xx[0] = t</span>
        <span class="s0">y = val * val * val - compensation_xxx[0]</span>
        <span class="s0">t = xxx[0] + y</span>
        <span class="s0">compensation_xxx[0] = t - xxx[0] - y</span>
        <span class="s0">xxx[0] = t</span>
        <span class="s0">y = val * val * val * val - compensation_xxxx[0]</span>
        <span class="s0">t = xxxx[0] + y</span>
        <span class="s0">compensation_xxxx[0] = t - xxxx[0] - y</span>
        <span class="s0">xxxx[0] = t</span>


<span class="s0">cdef inline void remove_kurt(float64_t val, int64_t *nobs,</span>
                             <span class="s0">float64_t *x, float64_t *xx,</span>
                             <span class="s0">float64_t *xxx, float64_t *xxxx,</span>
                             <span class="s0">float64_t *compensation_x,</span>
                             <span class="s0">float64_t *compensation_xx,</span>
                             <span class="s0">float64_t *compensation_xxx,</span>
                             <span class="s0">float64_t *compensation_xxxx) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the kurotic calc &quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t y, t</span>

    <span class="s0"># Not NaN</span>
    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>

        <span class="s0">y = - val - compensation_x[0]</span>
        <span class="s0">t = x[0] + y</span>
        <span class="s0">compensation_x[0] = t - x[0] - y</span>
        <span class="s0">x[0] = t</span>
        <span class="s0">y = - val * val - compensation_xx[0]</span>
        <span class="s0">t = xx[0] + y</span>
        <span class="s0">compensation_xx[0] = t - xx[0] - y</span>
        <span class="s0">xx[0] = t</span>
        <span class="s0">y = - val * val * val - compensation_xxx[0]</span>
        <span class="s0">t = xxx[0] + y</span>
        <span class="s0">compensation_xxx[0] = t - xxx[0] - y</span>
        <span class="s0">xxx[0] = t</span>
        <span class="s0">y = - val * val * val * val - compensation_xxxx[0]</span>
        <span class="s0">t = xxxx[0] + y</span>
        <span class="s0">compensation_xxxx[0] = t - xxxx[0] - y</span>
        <span class="s0">xxxx[0] = t</span>


<span class="s0">def roll_kurt(ndarray[float64_t] values, ndarray[int64_t] start,</span>
              <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j</span>
        <span class="s0">float64_t val, prev, mean_val, min_val, sum_val = 0</span>
        <span class="s0">float64_t compensation_xxxx_add = 0, compensation_xxxx_remove = 0</span>
        <span class="s0">float64_t compensation_xxx_remove = 0, compensation_xxx_add = 0</span>
        <span class="s0">float64_t compensation_xx_remove = 0, compensation_xx_add = 0</span>
        <span class="s0">float64_t compensation_x_remove = 0, compensation_x_add = 0</span>
        <span class="s0">float64_t x = 0, xx = 0, xxx = 0, xxxx = 0</span>
        <span class="s0">int64_t nobs = 0, s, e, N = len(values), nobs_mean = 0</span>
        <span class="s0">ndarray[float64_t] output, values_copy</span>
        <span class="s0">bint is_monotonic_increasing_bounds</span>

    <span class="s0">minp = max(minp, 4)</span>
    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>
    <span class="s0">values_copy = np.copy(values)</span>
    <span class="s0">min_val = np.nanmin(values)</span>

    <span class="s0">with nogil:</span>
        <span class="s0">for i in range(0, N):</span>
            <span class="s0">val = values_copy[i]</span>
            <span class="s0">if notnan(val):</span>
                <span class="s0">nobs_mean += 1</span>
                <span class="s0">sum_val += val</span>
        <span class="s0">mean_val = sum_val / nobs_mean</span>
        <span class="s0"># Other cases would lead to imprecision for smallest values</span>
        <span class="s0">if min_val - mean_val &gt; -1e4:</span>
            <span class="s0">mean_val = round(mean_val)</span>
            <span class="s0">for i in range(0, N):</span>
                <span class="s0">values_copy[i] = values_copy[i] - mean_val</span>

        <span class="s0">for i in range(0, N):</span>

            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0"># Over the first window, observations can only be added</span>
            <span class="s0"># never removed</span>
            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">add_kurt(values_copy[j], &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;xxxx,</span>
                             <span class="s0">&amp;compensation_x_add, &amp;compensation_xx_add,</span>
                             <span class="s0">&amp;compensation_xxx_add, &amp;compensation_xxxx_add)</span>

            <span class="s0">else:</span>

                <span class="s0"># After the first window, observations can both be added</span>
                <span class="s0"># and removed</span>
                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">remove_kurt(values_copy[j], &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;xxxx,</span>
                                <span class="s0">&amp;compensation_x_remove, &amp;compensation_xx_remove,</span>
                                <span class="s0">&amp;compensation_xxx_remove, &amp;compensation_xxxx_remove)</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">add_kurt(values_copy[j], &amp;nobs, &amp;x, &amp;xx, &amp;xxx, &amp;xxxx,</span>
                             <span class="s0">&amp;compensation_x_add, &amp;compensation_xx_add,</span>
                             <span class="s0">&amp;compensation_xxx_add, &amp;compensation_xxxx_add)</span>

            <span class="s0">output[i] = calc_kurt(minp, nobs, x, xx, xxx, xxxx)</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0</span>
                <span class="s0">x = 0.0</span>
                <span class="s0">xx = 0.0</span>
                <span class="s0">xxx = 0.0</span>
                <span class="s0">xxxx = 0.0</span>

    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling median, min, max</span>


<span class="s0">def roll_median_c(const float64_t[:] values, ndarray[int64_t] start,</span>
                  <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j</span>
        <span class="s0">bint err = False, is_monotonic_increasing_bounds</span>
        <span class="s0">int midpoint, ret = 0</span>
        <span class="s0">int64_t nobs = 0, N = len(values), s, e, win</span>
        <span class="s0">float64_t val, res, prev</span>
        <span class="s0">skiplist_t *sl</span>
        <span class="s0">ndarray[float64_t] output</span>

    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>

    <span class="s0"># we use the Fixed/Variable Indexer here as the</span>
    <span class="s0"># actual skiplist ops outweigh any window computation costs</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">if (end - start).max() == 0:</span>
        <span class="s0">output[:] = NaN</span>
        <span class="s0">return output</span>
    <span class="s0">win = (end - start).max()</span>
    <span class="s0">sl = skiplist_init(&lt;int&gt;win)</span>
    <span class="s0">if sl == NULL:</span>
        <span class="s0">raise MemoryError(&quot;skiplist_init failed&quot;)</span>

    <span class="s0">with nogil:</span>

        <span class="s0">for i in range(0, N):</span>
            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>

                <span class="s0"># setup</span>
                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">err = skiplist_insert(sl, val) == -1</span>
                        <span class="s0">if err:</span>
                            <span class="s0">break</span>

            <span class="s0">else:</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">err = skiplist_insert(sl, val) == -1</span>
                        <span class="s0">if err:</span>
                            <span class="s0">break</span>

                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">skiplist_remove(sl, val)</span>
                        <span class="s0">nobs -= 1</span>
            <span class="s0">if nobs &gt;= minp:</span>
                <span class="s0">midpoint = &lt;int&gt;(nobs / 2)</span>
                <span class="s0">if nobs % 2:</span>
                    <span class="s0">res = skiplist_get(sl, midpoint, &amp;ret)</span>
                <span class="s0">else:</span>
                    <span class="s0">res = (skiplist_get(sl, midpoint, &amp;ret) +</span>
                           <span class="s0">skiplist_get(sl, (midpoint - 1), &amp;ret)) / 2</span>
                <span class="s0">if ret == 0:</span>
                    <span class="s0">res = NaN</span>
            <span class="s0">else:</span>
                <span class="s0">res = NaN</span>

            <span class="s0">output[i] = res</span>

            <span class="s0">if not is_monotonic_increasing_bounds:</span>
                <span class="s0">nobs = 0</span>
                <span class="s0">skiplist_destroy(sl)</span>
                <span class="s0">sl = skiplist_init(&lt;int&gt;win)</span>

    <span class="s0">skiplist_destroy(sl)</span>
    <span class="s0">if err:</span>
        <span class="s0">raise MemoryError(&quot;skiplist_insert failed&quot;)</span>
    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>

<span class="s0"># Moving maximum / minimum code taken from Bottleneck under the terms</span>
<span class="s0"># of its Simplified BSD license</span>
<span class="s0"># https://github.com/pydata/bottleneck</span>


<span class="s0">cdef inline numeric_t init_mm(numeric_t ai, Py_ssize_t *nobs, bint is_max) nogil:</span>

    <span class="s0">if numeric_t in cython.floating:</span>
        <span class="s0">if ai == ai:</span>
            <span class="s0">nobs[0] = nobs[0] + 1</span>
        <span class="s0">elif is_max:</span>
            <span class="s0">if numeric_t == cython.float:</span>
                <span class="s0">ai = MINfloat32</span>
            <span class="s0">else:</span>
                <span class="s0">ai = MINfloat64</span>
        <span class="s0">else:</span>
            <span class="s0">if numeric_t == cython.float:</span>
                <span class="s0">ai = MAXfloat32</span>
            <span class="s0">else:</span>
                <span class="s0">ai = MAXfloat64</span>

    <span class="s0">else:</span>
        <span class="s0">nobs[0] = nobs[0] + 1</span>

    <span class="s0">return ai</span>


<span class="s0">cdef inline void remove_mm(numeric_t aold, Py_ssize_t *nobs) nogil:</span>
    <span class="s0">&quot;&quot;&quot; remove a value from the mm calc &quot;&quot;&quot;</span>
    <span class="s0">if numeric_t in cython.floating and aold == aold:</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>


<span class="s0">cdef inline numeric_t calc_mm(int64_t minp, Py_ssize_t nobs,</span>
                              <span class="s0">numeric_t value) nogil:</span>
    <span class="s0">cdef:</span>
        <span class="s0">numeric_t result</span>

    <span class="s0">if numeric_t in cython.floating:</span>
        <span class="s0">if nobs &gt;= minp:</span>
            <span class="s0">result = value</span>
        <span class="s0">else:</span>
            <span class="s0">result = NaN</span>
    <span class="s0">else:</span>
        <span class="s0">result = value</span>

    <span class="s0">return result</span>


<span class="s0">def roll_max(ndarray[float64_t] values, ndarray[int64_t] start,</span>
             <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Moving max of 1d array of any numeric type along axis=0 ignoring NaNs.</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">values : np.ndarray[np.float64]</span>
    <span class="s0">window : int, size of rolling window</span>
    <span class="s0">minp : if number of observations in window</span>
          <span class="s0">is below this, output a NaN</span>
    <span class="s0">index : ndarray, optional</span>
       <span class="s0">index for window computation</span>
    <span class="s0">closed : 'right', 'left', 'both', 'neither'</span>
            <span class="s0">make the interval closed on the right, left,</span>
            <span class="s0">both or neither endpoints</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">np.ndarray[float]</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">return _roll_min_max(values, start, end, minp, is_max=1)</span>


<span class="s0">def roll_min(ndarray[float64_t] values, ndarray[int64_t] start,</span>
             <span class="s0">ndarray[int64_t] end, int64_t minp) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Moving min of 1d array of any numeric type along axis=0 ignoring NaNs.</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">values : np.ndarray[np.float64]</span>
    <span class="s0">window : int, size of rolling window</span>
    <span class="s0">minp : if number of observations in window</span>
          <span class="s0">is below this, output a NaN</span>
    <span class="s0">index : ndarray, optional</span>
       <span class="s0">index for window computation</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">np.ndarray[float]</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">return _roll_min_max(values, start, end, minp, is_max=0)</span>


<span class="s0">cdef _roll_min_max(ndarray[numeric_t] values,</span>
                   <span class="s0">ndarray[int64_t] starti,</span>
                   <span class="s0">ndarray[int64_t] endi,</span>
                   <span class="s0">int64_t minp,</span>
                   <span class="s0">bint is_max):</span>
    <span class="s0">cdef:</span>
        <span class="s0">numeric_t ai</span>
        <span class="s0">int64_t curr_win_size, start</span>
        <span class="s0">Py_ssize_t i, k, nobs = 0, N = len(values)</span>
        <span class="s0">deque Q[int64_t]  # min/max always the front</span>
        <span class="s0">deque W[int64_t]  # track the whole window for nobs compute</span>
        <span class="s0">ndarray[float64_t, ndim=1] output</span>

    <span class="s0">output = np.empty(N, dtype=np.float64)</span>
    <span class="s0">Q = deque[int64_t]()</span>
    <span class="s0">W = deque[int64_t]()</span>

    <span class="s0">with nogil:</span>

        <span class="s0"># This is using a modified version of the C++ code in this</span>
        <span class="s0"># SO post: https://stackoverflow.com/a/12239580</span>
        <span class="s0"># The original impl didn't deal with variable window sizes</span>
        <span class="s0"># So the code was optimized for that</span>

        <span class="s0"># first window's size</span>
        <span class="s0">curr_win_size = endi[0] - starti[0]</span>
        <span class="s0"># GH 32865</span>
        <span class="s0"># Anchor output index to values index to provide custom</span>
        <span class="s0"># BaseIndexer support</span>
        <span class="s0">for i in range(N):</span>

            <span class="s0">curr_win_size = endi[i] - starti[i]</span>
            <span class="s0">if i == 0:</span>
                <span class="s0">start = starti[i]</span>
            <span class="s0">else:</span>
                <span class="s0">start = endi[i - 1]</span>

            <span class="s0">for k in range(start, endi[i]):</span>
                <span class="s0">ai = init_mm(values[k], &amp;nobs, is_max)</span>
                <span class="s0"># Discard previous entries if we find new min or max</span>
                <span class="s0">if is_max:</span>
                    <span class="s0">while not Q.empty() and ((ai &gt;= values[Q.back()]) or</span>
                                             <span class="s0">values[Q.back()] != values[Q.back()]):</span>
                        <span class="s0">Q.pop_back()</span>
                <span class="s0">else:</span>
                    <span class="s0">while not Q.empty() and ((ai &lt;= values[Q.back()]) or</span>
                                             <span class="s0">values[Q.back()] != values[Q.back()]):</span>
                        <span class="s0">Q.pop_back()</span>
                <span class="s0">Q.push_back(k)</span>
                <span class="s0">W.push_back(k)</span>

            <span class="s0"># Discard entries outside and left of current window</span>
            <span class="s0">while not Q.empty() and Q.front() &lt;= starti[i] - 1:</span>
                <span class="s0">Q.pop_front()</span>
            <span class="s0">while not W.empty() and W.front() &lt;= starti[i] - 1:</span>
                <span class="s0">remove_mm(values[W.front()], &amp;nobs)</span>
                <span class="s0">W.pop_front()</span>

            <span class="s0"># Save output based on index in input value array</span>
            <span class="s0">if not Q.empty() and curr_win_size &gt; 0:</span>
                <span class="s0">output[i] = calc_mm(minp, nobs, values[Q.front()])</span>
            <span class="s0">else:</span>
                <span class="s0">output[i] = NaN</span>

    <span class="s0">return output</span>


<span class="s0">cdef enum InterpolationType:</span>
    <span class="s0">LINEAR,</span>
    <span class="s0">LOWER,</span>
    <span class="s0">HIGHER,</span>
    <span class="s0">NEAREST,</span>
    <span class="s0">MIDPOINT</span>


<span class="s0">interpolation_types = {</span>
    <span class="s0">'linear': LINEAR,</span>
    <span class="s0">'lower': LOWER,</span>
    <span class="s0">'higher': HIGHER,</span>
    <span class="s0">'nearest': NEAREST,</span>
    <span class="s0">'midpoint': MIDPOINT,</span>
<span class="s0">}</span>


<span class="s0">def roll_quantile(const float64_t[:] values, ndarray[int64_t] start,</span>
                  <span class="s0">ndarray[int64_t] end, int64_t minp,</span>
                  <span class="s0">float64_t quantile, str interpolation) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">O(N log(window)) implementation using skip list</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j, s, e, N = len(values), idx</span>
        <span class="s0">int ret = 0</span>
        <span class="s0">int64_t nobs = 0, win</span>
        <span class="s0">float64_t val, prev, midpoint, idx_with_fraction</span>
        <span class="s0">float64_t vlow, vhigh</span>
        <span class="s0">skiplist_t *skiplist</span>
        <span class="s0">InterpolationType interpolation_type</span>
        <span class="s0">ndarray[float64_t] output</span>

    <span class="s0">if quantile &lt;= 0.0 or quantile &gt;= 1.0:</span>
        <span class="s0">raise ValueError(f&quot;quantile value {quantile} not in [0, 1]&quot;)</span>

    <span class="s0">try:</span>
        <span class="s0">interpolation_type = interpolation_types[interpolation]</span>
    <span class="s0">except KeyError:</span>
        <span class="s0">raise ValueError(f&quot;Interpolation '{interpolation}' is not supported&quot;)</span>

    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0"># we use the Fixed/Variable Indexer here as the</span>
    <span class="s0"># actual skiplist ops outweigh any window computation costs</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">win = (end - start).max()</span>
    <span class="s0">if win == 0:</span>
        <span class="s0">output[:] = NaN</span>
        <span class="s0">return output</span>
    <span class="s0">skiplist = skiplist_init(&lt;int&gt;win)</span>
    <span class="s0">if skiplist == NULL:</span>
        <span class="s0">raise MemoryError(&quot;skiplist_init failed&quot;)</span>

    <span class="s0">with nogil:</span>
        <span class="s0">for i in range(0, N):</span>
            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>
                <span class="s0">if not is_monotonic_increasing_bounds:</span>
                    <span class="s0">nobs = 0</span>
                    <span class="s0">skiplist_destroy(skiplist)</span>
                    <span class="s0">skiplist = skiplist_init(&lt;int&gt;win)</span>

                <span class="s0"># setup</span>
                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">skiplist_insert(skiplist, val)</span>

            <span class="s0">else:</span>
                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">skiplist_insert(skiplist, val)</span>

                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">val = values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">skiplist_remove(skiplist, val)</span>
                        <span class="s0">nobs -= 1</span>
            <span class="s0">if nobs &gt;= minp:</span>
                <span class="s0">if nobs == 1:</span>
                    <span class="s0"># Single value in skip list</span>
                    <span class="s0">output[i] = skiplist_get(skiplist, 0, &amp;ret)</span>
                <span class="s0">else:</span>
                    <span class="s0">idx_with_fraction = quantile * (nobs - 1)</span>
                    <span class="s0">idx = &lt;int&gt;idx_with_fraction</span>

                    <span class="s0">if idx_with_fraction == idx:</span>
                        <span class="s0"># no need to interpolate</span>
                        <span class="s0">output[i] = skiplist_get(skiplist, idx, &amp;ret)</span>
                        <span class="s0">continue</span>

                    <span class="s0">if interpolation_type == LINEAR:</span>
                        <span class="s0">vlow = skiplist_get(skiplist, idx, &amp;ret)</span>
                        <span class="s0">vhigh = skiplist_get(skiplist, idx + 1, &amp;ret)</span>
                        <span class="s0">output[i] = ((vlow + (vhigh - vlow) *</span>
                                      <span class="s0">(idx_with_fraction - idx)))</span>
                    <span class="s0">elif interpolation_type == LOWER:</span>
                        <span class="s0">output[i] = skiplist_get(skiplist, idx, &amp;ret)</span>
                    <span class="s0">elif interpolation_type == HIGHER:</span>
                        <span class="s0">output[i] = skiplist_get(skiplist, idx + 1, &amp;ret)</span>
                    <span class="s0">elif interpolation_type == NEAREST:</span>
                        <span class="s0"># the same behaviour as round()</span>
                        <span class="s0">if idx_with_fraction - idx == 0.5:</span>
                            <span class="s0">if idx % 2 == 0:</span>
                                <span class="s0">output[i] = skiplist_get(skiplist, idx, &amp;ret)</span>
                            <span class="s0">else:</span>
                                <span class="s0">output[i] = skiplist_get(</span>
                                    <span class="s0">skiplist, idx + 1, &amp;ret)</span>
                        <span class="s0">elif idx_with_fraction - idx &lt; 0.5:</span>
                            <span class="s0">output[i] = skiplist_get(skiplist, idx, &amp;ret)</span>
                        <span class="s0">else:</span>
                            <span class="s0">output[i] = skiplist_get(skiplist, idx + 1, &amp;ret)</span>
                    <span class="s0">elif interpolation_type == MIDPOINT:</span>
                        <span class="s0">vlow = skiplist_get(skiplist, idx, &amp;ret)</span>
                        <span class="s0">vhigh = skiplist_get(skiplist, idx + 1, &amp;ret)</span>
                        <span class="s0">output[i] = &lt;float64_t&gt;(vlow + vhigh) / 2</span>

                    <span class="s0">if ret == 0:</span>
                        <span class="s0">output[i] = NaN</span>
            <span class="s0">else:</span>
                <span class="s0">output[i] = NaN</span>

    <span class="s0">skiplist_destroy(skiplist)</span>

    <span class="s0">return output</span>


<span class="s0">rolling_rank_tiebreakers = {</span>
    <span class="s0">&quot;average&quot;: TiebreakEnumType.TIEBREAK_AVERAGE,</span>
    <span class="s0">&quot;min&quot;: TiebreakEnumType.TIEBREAK_MIN,</span>
    <span class="s0">&quot;max&quot;: TiebreakEnumType.TIEBREAK_MAX,</span>
<span class="s0">}</span>


<span class="s0">def roll_rank(const float64_t[:] values, ndarray[int64_t] start,</span>
              <span class="s0">ndarray[int64_t] end, int64_t minp, bint percentile,</span>
              <span class="s0">str method, bint ascending) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">O(N log(window)) implementation using skip list</span>

    <span class="s0">derived from roll_quantile</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j, s, e, N = len(values), idx</span>
        <span class="s0">float64_t rank_min = 0, rank = 0</span>
        <span class="s0">int64_t nobs = 0, win</span>
        <span class="s0">float64_t val</span>
        <span class="s0">skiplist_t *skiplist</span>
        <span class="s0">float64_t[::1] output</span>
        <span class="s0">TiebreakEnumType rank_type</span>

    <span class="s0">try:</span>
        <span class="s0">rank_type = rolling_rank_tiebreakers[method]</span>
    <span class="s0">except KeyError:</span>
        <span class="s0">raise ValueError(f&quot;Method '{method}' is not supported&quot;)</span>

    <span class="s0">is_monotonic_increasing_bounds = is_monotonic_increasing_start_end_bounds(</span>
        <span class="s0">start, end</span>
    <span class="s0">)</span>
    <span class="s0"># we use the Fixed/Variable Indexer here as the</span>
    <span class="s0"># actual skiplist ops outweigh any window computation costs</span>
    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">win = (end - start).max()</span>
    <span class="s0">if win == 0:</span>
        <span class="s0">output[:] = NaN</span>
        <span class="s0">return np.asarray(output)</span>
    <span class="s0">skiplist = skiplist_init(&lt;int&gt;win)</span>
    <span class="s0">if skiplist == NULL:</span>
        <span class="s0">raise MemoryError(&quot;skiplist_init failed&quot;)</span>

    <span class="s0">with nogil:</span>
        <span class="s0">for i in range(N):</span>
            <span class="s0">s = start[i]</span>
            <span class="s0">e = end[i]</span>

            <span class="s0">if i == 0 or not is_monotonic_increasing_bounds:</span>
                <span class="s0">if not is_monotonic_increasing_bounds:</span>
                    <span class="s0">nobs = 0</span>
                    <span class="s0">skiplist_destroy(skiplist)</span>
                    <span class="s0">skiplist = skiplist_init(&lt;int&gt;win)</span>

                <span class="s0"># setup</span>
                <span class="s0">for j in range(s, e):</span>
                    <span class="s0">val = values[j] if ascending else -values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">rank = skiplist_insert(skiplist, val)</span>
                        <span class="s0">if rank == -1:</span>
                            <span class="s0">raise MemoryError(&quot;skiplist_insert failed&quot;)</span>
                        <span class="s0">if rank_type == TiebreakEnumType.TIEBREAK_AVERAGE:</span>
                            <span class="s0"># The average rank of `val` is the sum of the ranks of all</span>
                            <span class="s0"># instances of `val` in the skip list divided by the number</span>
                            <span class="s0"># of instances. The sum of consecutive integers from 1 to N</span>
                            <span class="s0"># is N * (N + 1) / 2.</span>
                            <span class="s0"># The sum of the ranks is the sum of integers from the</span>
                            <span class="s0"># lowest rank to the highest rank, which is the sum of</span>
                            <span class="s0"># integers from 1 to the highest rank minus the sum of</span>
                            <span class="s0"># integers from 1 to one less than the lowest rank.</span>
                            <span class="s0">rank_min = skiplist_min_rank(skiplist, val)</span>
                            <span class="s0">rank = (((rank * (rank + 1) / 2)</span>
                                    <span class="s0">- ((rank_min - 1) * rank_min / 2))</span>
                                    <span class="s0">/ (rank - rank_min + 1))</span>
                        <span class="s0">elif rank_type == TiebreakEnumType.TIEBREAK_MIN:</span>
                            <span class="s0">rank = skiplist_min_rank(skiplist, val)</span>
                    <span class="s0">else:</span>
                        <span class="s0">rank = NaN</span>

            <span class="s0">else:</span>
                <span class="s0"># calculate deletes</span>
                <span class="s0">for j in range(start[i - 1], s):</span>
                    <span class="s0">val = values[j] if ascending else -values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">skiplist_remove(skiplist, val)</span>
                        <span class="s0">nobs -= 1</span>

                <span class="s0"># calculate adds</span>
                <span class="s0">for j in range(end[i - 1], e):</span>
                    <span class="s0">val = values[j] if ascending else -values[j]</span>
                    <span class="s0">if notnan(val):</span>
                        <span class="s0">nobs += 1</span>
                        <span class="s0">rank = skiplist_insert(skiplist, val)</span>
                        <span class="s0">if rank == -1:</span>
                            <span class="s0">raise MemoryError(&quot;skiplist_insert failed&quot;)</span>
                        <span class="s0">if rank_type == TiebreakEnumType.TIEBREAK_AVERAGE:</span>
                            <span class="s0">rank_min = skiplist_min_rank(skiplist, val)</span>
                            <span class="s0">rank = (((rank * (rank + 1) / 2)</span>
                                    <span class="s0">- ((rank_min - 1) * rank_min / 2))</span>
                                    <span class="s0">/ (rank - rank_min + 1))</span>
                        <span class="s0">elif rank_type == TiebreakEnumType.TIEBREAK_MIN:</span>
                            <span class="s0">rank = skiplist_min_rank(skiplist, val)</span>
                    <span class="s0">else:</span>
                        <span class="s0">rank = NaN</span>
            <span class="s0">if nobs &gt;= minp:</span>
                <span class="s0">output[i] = rank / nobs if percentile else rank</span>
            <span class="s0">else:</span>
                <span class="s0">output[i] = NaN</span>

    <span class="s0">skiplist_destroy(skiplist)</span>

    <span class="s0">return np.asarray(output)</span>


<span class="s0">def roll_apply(object obj,</span>
               <span class="s0">ndarray[int64_t] start, ndarray[int64_t] end,</span>
               <span class="s0">int64_t minp,</span>
               <span class="s0">object function, bint raw,</span>
               <span class="s0">tuple args, dict kwargs) -&gt; np.ndarray:</span>
    <span class="s0">cdef:</span>
        <span class="s0">ndarray[float64_t] output, counts</span>
        <span class="s0">ndarray[float64_t, cast=True] arr</span>
        <span class="s0">Py_ssize_t i, s, e, N = len(start), n = len(obj)</span>

    <span class="s0">if n == 0:</span>
        <span class="s0">return np.array([], dtype=np.float64)</span>

    <span class="s0">arr = np.asarray(obj)</span>

    <span class="s0"># ndarray input</span>
    <span class="s0">if raw and not arr.flags.c_contiguous:</span>
        <span class="s0">arr = arr.copy('C')</span>

    <span class="s0">counts = roll_sum(np.isfinite(arr).astype(float), start, end, minp)</span>

    <span class="s0">output = np.empty(N, dtype=np.float64)</span>

    <span class="s0">for i in range(N):</span>

        <span class="s0">s = start[i]</span>
        <span class="s0">e = end[i]</span>

        <span class="s0">if counts[i] &gt;= minp:</span>
            <span class="s0">if raw:</span>
                <span class="s0">output[i] = function(arr[s:e], *args, **kwargs)</span>
            <span class="s0">else:</span>
                <span class="s0">output[i] = function(obj.iloc[s:e], *args, **kwargs)</span>
        <span class="s0">else:</span>
            <span class="s0">output[i] = NaN</span>

    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling sum and mean for weighted window</span>


<span class="s0">def roll_weighted_sum(</span>
    <span class="s0">const float64_t[:] values, const float64_t[:] weights, int minp</span>
<span class="s0">) -&gt; np.ndaray:</span>
    <span class="s0">return _roll_weighted_sum_mean(values, weights, minp, avg=0)</span>


<span class="s0">def roll_weighted_mean(</span>
    <span class="s0">const float64_t[:] values, const float64_t[:] weights, int minp</span>
<span class="s0">) -&gt; np.ndaray:</span>
    <span class="s0">return _roll_weighted_sum_mean(values, weights, minp, avg=1)</span>


<span class="s0">cdef float64_t[:] _roll_weighted_sum_mean(const float64_t[:] values,</span>
                                          <span class="s0">const float64_t[:] weights,</span>
                                          <span class="s0">int minp, bint avg):</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Assume len(weights) &lt;&lt; len(values)</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">cdef:</span>
        <span class="s0">float64_t[:] output, tot_wgt, counts</span>
        <span class="s0">Py_ssize_t in_i, win_i, win_n, in_n</span>
        <span class="s0">float64_t val_in, val_win, c, w</span>

    <span class="s0">in_n = len(values)</span>
    <span class="s0">win_n = len(weights)</span>

    <span class="s0">output = np.zeros(in_n, dtype=np.float64)</span>
    <span class="s0">counts = np.zeros(in_n, dtype=np.float64)</span>
    <span class="s0">if avg:</span>
        <span class="s0">tot_wgt = np.zeros(in_n, dtype=np.float64)</span>

    <span class="s0">elif minp &gt; in_n:</span>
        <span class="s0">minp = in_n + 1</span>

    <span class="s0">minp = max(minp, 1)</span>

    <span class="s0">with nogil:</span>
        <span class="s0">if avg:</span>
            <span class="s0">for win_i in range(win_n):</span>
                <span class="s0">val_win = weights[win_i]</span>
                <span class="s0">if val_win != val_win:</span>
                    <span class="s0">continue</span>

                <span class="s0">for in_i in range(in_n - (win_n - win_i) + 1):</span>
                    <span class="s0">val_in = values[in_i]</span>
                    <span class="s0">if val_in == val_in:</span>
                        <span class="s0">output[in_i + (win_n - win_i) - 1] += val_in * val_win</span>
                        <span class="s0">counts[in_i + (win_n - win_i) - 1] += 1</span>
                        <span class="s0">tot_wgt[in_i + (win_n - win_i) - 1] += val_win</span>

            <span class="s0">for in_i in range(in_n):</span>
                <span class="s0">c = counts[in_i]</span>
                <span class="s0">if c &lt; minp:</span>
                    <span class="s0">output[in_i] = NaN</span>
                <span class="s0">else:</span>
                    <span class="s0">w = tot_wgt[in_i]</span>
                    <span class="s0">if w == 0:</span>
                        <span class="s0">output[in_i] = NaN</span>
                    <span class="s0">else:</span>
                        <span class="s0">output[in_i] /= tot_wgt[in_i]</span>

        <span class="s0">else:</span>
            <span class="s0">for win_i in range(win_n):</span>
                <span class="s0">val_win = weights[win_i]</span>
                <span class="s0">if val_win != val_win:</span>
                    <span class="s0">continue</span>

                <span class="s0">for in_i in range(in_n - (win_n - win_i) + 1):</span>
                    <span class="s0">val_in = values[in_i]</span>

                    <span class="s0">if val_in == val_in:</span>
                        <span class="s0">output[in_i + (win_n - win_i) - 1] += val_in * val_win</span>
                        <span class="s0">counts[in_i + (win_n - win_i) - 1] += 1</span>

            <span class="s0">for in_i in range(in_n):</span>
                <span class="s0">c = counts[in_i]</span>
                <span class="s0">if c &lt; minp:</span>
                    <span class="s0">output[in_i] = NaN</span>

    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Rolling var for weighted window</span>


<span class="s0">cdef inline float64_t calc_weighted_var(float64_t t,</span>
                                        <span class="s0">float64_t sum_w,</span>
                                        <span class="s0">Py_ssize_t win_n,</span>
                                        <span class="s0">unsigned int ddof,</span>
                                        <span class="s0">float64_t nobs,</span>
                                        <span class="s0">int64_t minp) nogil:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Calculate weighted variance for a window using West's method.</span>

    <span class="s0">Paper: https://dl.acm.org/citation.cfm?id=359153</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">t: float64_t</span>
        <span class="s0">sum of weighted squared differences</span>
    <span class="s0">sum_w: float64_t</span>
        <span class="s0">sum of weights</span>
    <span class="s0">win_n: Py_ssize_t</span>
        <span class="s0">window size</span>
    <span class="s0">ddof: unsigned int</span>
        <span class="s0">delta degrees of freedom</span>
    <span class="s0">nobs: float64_t</span>
        <span class="s0">number of observations</span>
    <span class="s0">minp: int64_t</span>
        <span class="s0">minimum number of observations</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">result : float64_t</span>
        <span class="s0">weighted variance of the window</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t result</span>

    <span class="s0"># Variance is unchanged if no observation is added or removed</span>
    <span class="s0">if (nobs &gt;= minp) and (nobs &gt; ddof):</span>

        <span class="s0"># pathological case</span>
        <span class="s0">if nobs == 1:</span>
            <span class="s0">result = 0</span>
        <span class="s0">else:</span>
            <span class="s0">result = t * win_n / ((win_n - ddof) * sum_w)</span>
            <span class="s0">if result &lt; 0:</span>
                <span class="s0">result = 0</span>
    <span class="s0">else:</span>
        <span class="s0">result = NaN</span>

    <span class="s0">return result</span>


<span class="s0">cdef inline void add_weighted_var(float64_t val,</span>
                                  <span class="s0">float64_t w,</span>
                                  <span class="s0">float64_t *t,</span>
                                  <span class="s0">float64_t *sum_w,</span>
                                  <span class="s0">float64_t *mean,</span>
                                  <span class="s0">float64_t *nobs) nogil:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Update weighted mean, sum of weights and sum of weighted squared</span>
    <span class="s0">differences to include value and weight pair in weighted variance</span>
    <span class="s0">calculation using West's method.</span>

    <span class="s0">Paper: https://dl.acm.org/citation.cfm?id=359153</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">val: float64_t</span>
        <span class="s0">window values</span>
    <span class="s0">w: float64_t</span>
        <span class="s0">window weights</span>
    <span class="s0">t: float64_t</span>
        <span class="s0">sum of weighted squared differences</span>
    <span class="s0">sum_w: float64_t</span>
        <span class="s0">sum of weights</span>
    <span class="s0">mean: float64_t</span>
        <span class="s0">weighted mean</span>
    <span class="s0">nobs: float64_t</span>
        <span class="s0">number of observations</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t temp, q, r</span>

    <span class="s0">if isnan(val):</span>
        <span class="s0">return</span>

    <span class="s0">nobs[0] = nobs[0] + 1</span>

    <span class="s0">q = val - mean[0]</span>
    <span class="s0">temp = sum_w[0] + w</span>
    <span class="s0">r = q * w / temp</span>

    <span class="s0">mean[0] = mean[0] + r</span>
    <span class="s0">t[0] = t[0] + r * sum_w[0] * q</span>
    <span class="s0">sum_w[0] = temp</span>


<span class="s0">cdef inline void remove_weighted_var(float64_t val,</span>
                                     <span class="s0">float64_t w,</span>
                                     <span class="s0">float64_t *t,</span>
                                     <span class="s0">float64_t *sum_w,</span>
                                     <span class="s0">float64_t *mean,</span>
                                     <span class="s0">float64_t *nobs) nogil:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Update weighted mean, sum of weights and sum of weighted squared</span>
    <span class="s0">differences to remove value and weight pair from weighted variance</span>
    <span class="s0">calculation using West's method.</span>

    <span class="s0">Paper: https://dl.acm.org/citation.cfm?id=359153</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">val: float64_t</span>
        <span class="s0">window values</span>
    <span class="s0">w: float64_t</span>
        <span class="s0">window weights</span>
    <span class="s0">t: float64_t</span>
        <span class="s0">sum of weighted squared differences</span>
    <span class="s0">sum_w: float64_t</span>
        <span class="s0">sum of weights</span>
    <span class="s0">mean: float64_t</span>
        <span class="s0">weighted mean</span>
    <span class="s0">nobs: float64_t</span>
        <span class="s0">number of observations</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t temp, q, r</span>

    <span class="s0">if notnan(val):</span>
        <span class="s0">nobs[0] = nobs[0] - 1</span>

        <span class="s0">if nobs[0]:</span>
            <span class="s0">q = val - mean[0]</span>
            <span class="s0">temp = sum_w[0] - w</span>
            <span class="s0">r = q * w / temp</span>

            <span class="s0">mean[0] = mean[0] - r</span>
            <span class="s0">t[0] = t[0] - r * sum_w[0] * q</span>
            <span class="s0">sum_w[0] = temp</span>

        <span class="s0">else:</span>
            <span class="s0">t[0] = 0</span>
            <span class="s0">sum_w[0] = 0</span>
            <span class="s0">mean[0] = 0</span>


<span class="s0">def roll_weighted_var(const float64_t[:] values, const float64_t[:] weights,</span>
                      <span class="s0">int64_t minp, unsigned int ddof):</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Calculates weighted rolling variance using West's online algorithm.</span>

    <span class="s0">Paper: https://dl.acm.org/citation.cfm?id=359153</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">values: float64_t[:]</span>
        <span class="s0">values to roll window over</span>
    <span class="s0">weights: float64_t[:]</span>
        <span class="s0">array of weights whose length is window size</span>
    <span class="s0">minp: int64_t</span>
        <span class="s0">minimum number of observations to calculate</span>
        <span class="s0">variance of a window</span>
    <span class="s0">ddof: unsigned int</span>
         <span class="s0">the divisor used in variance calculations</span>
         <span class="s0">is the window size - ddof</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">output: float64_t[:]</span>
        <span class="s0">weighted variances of windows</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">float64_t t = 0, sum_w = 0, mean = 0, nobs = 0</span>
        <span class="s0">float64_t val, pre_val, w, pre_w</span>
        <span class="s0">Py_ssize_t i, n, win_n</span>
        <span class="s0">float64_t[:] output</span>

    <span class="s0">n = len(values)</span>
    <span class="s0">win_n = len(weights)</span>
    <span class="s0">output = np.empty(n, dtype=np.float64)</span>

    <span class="s0">with nogil:</span>

        <span class="s0">for i in range(win_n):</span>
            <span class="s0">add_weighted_var(values[i], weights[i], &amp;t,</span>
                             <span class="s0">&amp;sum_w, &amp;mean, &amp;nobs)</span>

            <span class="s0">output[i] = calc_weighted_var(t, sum_w, win_n,</span>
                                          <span class="s0">ddof, nobs, minp)</span>

        <span class="s0">for i in range(win_n, n):</span>
            <span class="s0">val = values[i]</span>
            <span class="s0">pre_val = values[i - win_n]</span>

            <span class="s0">w = weights[i % win_n]</span>
            <span class="s0">pre_w = weights[(i - win_n) % win_n]</span>

            <span class="s0">if notnan(val):</span>
                <span class="s0">if pre_val == pre_val:</span>
                    <span class="s0">remove_weighted_var(pre_val, pre_w, &amp;t,</span>
                                        <span class="s0">&amp;sum_w, &amp;mean, &amp;nobs)</span>

                <span class="s0">add_weighted_var(val, w, &amp;t, &amp;sum_w, &amp;mean, &amp;nobs)</span>

            <span class="s0">elif pre_val == pre_val:</span>
                <span class="s0">remove_weighted_var(pre_val, pre_w, &amp;t,</span>
                                    <span class="s0">&amp;sum_w, &amp;mean, &amp;nobs)</span>

            <span class="s0">output[i] = calc_weighted_var(t, sum_w, win_n,</span>
                                          <span class="s0">ddof, nobs, minp)</span>

    <span class="s0">return output</span>


<span class="s0"># ----------------------------------------------------------------------</span>
<span class="s0"># Exponentially weighted moving</span>

<span class="s0">def ewm(const float64_t[:] vals, const int64_t[:] start, const int64_t[:] end,</span>
        <span class="s0">int minp, float64_t com, bint adjust, bint ignore_na,</span>
        <span class="s0">const float64_t[:] deltas=None, bint normalize=True) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Compute exponentially-weighted moving average or sum using center-of-mass.</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">vals : ndarray (float64 type)</span>
    <span class="s0">start: ndarray (int64 type)</span>
    <span class="s0">end: ndarray (int64 type)</span>
    <span class="s0">minp : int</span>
    <span class="s0">com : float64</span>
    <span class="s0">adjust : bool</span>
    <span class="s0">ignore_na : bool</span>
    <span class="s0">deltas : ndarray (float64 type), optional. If None, implicitly assumes equally</span>
             <span class="s0">spaced points (used when `times` is not passed)</span>
    <span class="s0">normalize : bool, optional.</span>
                <span class="s0">If True, calculate the mean. If False, calculate the sum.</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">np.ndarray[float64_t]</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j, s, e, nobs, win_size, N = len(vals), M = len(start)</span>
        <span class="s0">const float64_t[:] sub_vals</span>
        <span class="s0">const float64_t[:] sub_deltas=None</span>
        <span class="s0">ndarray[float64_t] sub_output, output = np.empty(N, dtype=np.float64)</span>
        <span class="s0">float64_t alpha, old_wt_factor, new_wt, weighted, old_wt, cur</span>
        <span class="s0">bint is_observation, use_deltas</span>

    <span class="s0">if N == 0:</span>
        <span class="s0">return output</span>

    <span class="s0">use_deltas = deltas is not None</span>

    <span class="s0">alpha = 1. / (1. + com)</span>
    <span class="s0">old_wt_factor = 1. - alpha</span>
    <span class="s0">new_wt = 1. if adjust else alpha</span>

    <span class="s0">for j in range(M):</span>
        <span class="s0">s = start[j]</span>
        <span class="s0">e = end[j]</span>
        <span class="s0">sub_vals = vals[s:e]</span>
        <span class="s0"># note that len(deltas) = len(vals) - 1 and deltas[i] is to be used in</span>
        <span class="s0"># conjunction with vals[i+1]</span>
        <span class="s0">if use_deltas:</span>
            <span class="s0">sub_deltas = deltas[s:e - 1]</span>
        <span class="s0">win_size = len(sub_vals)</span>
        <span class="s0">sub_output = np.empty(win_size, dtype=np.float64)</span>

        <span class="s0">weighted = sub_vals[0]</span>
        <span class="s0">is_observation = weighted == weighted</span>
        <span class="s0">nobs = int(is_observation)</span>
        <span class="s0">sub_output[0] = weighted if nobs &gt;= minp else NaN</span>
        <span class="s0">old_wt = 1.</span>

        <span class="s0">with nogil:</span>
            <span class="s0">for i in range(1, win_size):</span>
                <span class="s0">cur = sub_vals[i]</span>
                <span class="s0">is_observation = cur == cur</span>
                <span class="s0">nobs += is_observation</span>
                <span class="s0">if weighted == weighted:</span>

                    <span class="s0">if is_observation or not ignore_na:</span>
                        <span class="s0">if normalize:</span>
                            <span class="s0">if use_deltas:</span>
                                <span class="s0">old_wt *= old_wt_factor ** sub_deltas[i - 1]</span>
                            <span class="s0">else:</span>
                                <span class="s0">old_wt *= old_wt_factor</span>
                        <span class="s0">else:</span>
                            <span class="s0">weighted = old_wt_factor * weighted</span>
                        <span class="s0">if is_observation:</span>
                            <span class="s0">if normalize:</span>
                                <span class="s0"># avoid numerical errors on constant series</span>
                                <span class="s0">if weighted != cur:</span>
                                    <span class="s0">weighted = old_wt * weighted + new_wt * cur</span>
                                    <span class="s0">weighted /= (old_wt + new_wt)</span>
                                <span class="s0">if adjust:</span>
                                    <span class="s0">old_wt += new_wt</span>
                                <span class="s0">else:</span>
                                    <span class="s0">old_wt = 1.</span>
                            <span class="s0">else:</span>
                                <span class="s0">weighted += cur</span>
                <span class="s0">elif is_observation:</span>
                    <span class="s0">weighted = cur</span>

                <span class="s0">sub_output[i] = weighted if nobs &gt;= minp else NaN</span>

        <span class="s0">output[s:e] = sub_output</span>

    <span class="s0">return output</span>


<span class="s0">def ewmcov(const float64_t[:] input_x, const int64_t[:] start, const int64_t[:] end,</span>
           <span class="s0">int minp, const float64_t[:] input_y, float64_t com, bint adjust,</span>
           <span class="s0">bint ignore_na, bint bias) -&gt; np.ndarray:</span>
    <span class="s0">&quot;&quot;&quot;</span>
    <span class="s0">Compute exponentially-weighted moving variance using center-of-mass.</span>

    <span class="s0">Parameters</span>
    <span class="s0">----------</span>
    <span class="s0">input_x : ndarray (float64 type)</span>
    <span class="s0">start: ndarray (int64 type)</span>
    <span class="s0">end: ndarray (int64 type)</span>
    <span class="s0">minp : int</span>
    <span class="s0">input_y : ndarray (float64 type)</span>
    <span class="s0">com : float64</span>
    <span class="s0">adjust : bool</span>
    <span class="s0">ignore_na : bool</span>
    <span class="s0">bias : bool</span>

    <span class="s0">Returns</span>
    <span class="s0">-------</span>
    <span class="s0">np.ndarray[float64_t]</span>
    <span class="s0">&quot;&quot;&quot;</span>

    <span class="s0">cdef:</span>
        <span class="s0">Py_ssize_t i, j, s, e, win_size, nobs</span>
        <span class="s0">Py_ssize_t N = len(input_x), M = len(input_y), L = len(start)</span>
        <span class="s0">float64_t alpha, old_wt_factor, new_wt, mean_x, mean_y, cov</span>
        <span class="s0">float64_t sum_wt, sum_wt2, old_wt, cur_x, cur_y, old_mean_x, old_mean_y</span>
        <span class="s0">float64_t numerator, denominator</span>
        <span class="s0">const float64_t[:] sub_x_vals, sub_y_vals</span>
        <span class="s0">ndarray[float64_t] sub_out, output = np.empty(N, dtype=np.float64)</span>
        <span class="s0">bint is_observation</span>

    <span class="s0">if M != N:</span>
        <span class="s0">raise ValueError(f&quot;arrays are of different lengths ({N} and {M})&quot;)</span>

    <span class="s0">if N == 0:</span>
        <span class="s0">return output</span>

    <span class="s0">alpha = 1. / (1. + com)</span>
    <span class="s0">old_wt_factor = 1. - alpha</span>
    <span class="s0">new_wt = 1. if adjust else alpha</span>

    <span class="s0">for j in range(L):</span>
        <span class="s0">s = start[j]</span>
        <span class="s0">e = end[j]</span>
        <span class="s0">sub_x_vals = input_x[s:e]</span>
        <span class="s0">sub_y_vals = input_y[s:e]</span>
        <span class="s0">win_size = len(sub_x_vals)</span>
        <span class="s0">sub_out = np.empty(win_size, dtype=np.float64)</span>

        <span class="s0">mean_x = sub_x_vals[0]</span>
        <span class="s0">mean_y = sub_y_vals[0]</span>
        <span class="s0">is_observation = (mean_x == mean_x) and (mean_y == mean_y)</span>
        <span class="s0">nobs = int(is_observation)</span>
        <span class="s0">if not is_observation:</span>
            <span class="s0">mean_x = NaN</span>
            <span class="s0">mean_y = NaN</span>
        <span class="s0">sub_out[0] = (0. if bias else NaN) if nobs &gt;= minp else NaN</span>
        <span class="s0">cov = 0.</span>
        <span class="s0">sum_wt = 1.</span>
        <span class="s0">sum_wt2 = 1.</span>
        <span class="s0">old_wt = 1.</span>

        <span class="s0">with nogil:</span>
            <span class="s0">for i in range(1, win_size):</span>
                <span class="s0">cur_x = sub_x_vals[i]</span>
                <span class="s0">cur_y = sub_y_vals[i]</span>
                <span class="s0">is_observation = (cur_x == cur_x) and (cur_y == cur_y)</span>
                <span class="s0">nobs += is_observation</span>
                <span class="s0">if mean_x == mean_x:</span>
                    <span class="s0">if is_observation or not ignore_na:</span>
                        <span class="s0">sum_wt *= old_wt_factor</span>
                        <span class="s0">sum_wt2 *= (old_wt_factor * old_wt_factor)</span>
                        <span class="s0">old_wt *= old_wt_factor</span>
                        <span class="s0">if is_observation:</span>
                            <span class="s0">old_mean_x = mean_x</span>
                            <span class="s0">old_mean_y = mean_y</span>

                            <span class="s0"># avoid numerical errors on constant series</span>
                            <span class="s0">if mean_x != cur_x:</span>
                                <span class="s0">mean_x = ((old_wt * old_mean_x) +</span>
                                          <span class="s0">(new_wt * cur_x)) / (old_wt + new_wt)</span>

                            <span class="s0"># avoid numerical errors on constant series</span>
                            <span class="s0">if mean_y != cur_y:</span>
                                <span class="s0">mean_y = ((old_wt * old_mean_y) +</span>
                                          <span class="s0">(new_wt * cur_y)) / (old_wt + new_wt)</span>
                            <span class="s0">cov = ((old_wt * (cov + ((old_mean_x - mean_x) *</span>
                                                     <span class="s0">(old_mean_y - mean_y)))) +</span>
                                   <span class="s0">(new_wt * ((cur_x - mean_x) *</span>
                                              <span class="s0">(cur_y - mean_y)))) / (old_wt + new_wt)</span>
                            <span class="s0">sum_wt += new_wt</span>
                            <span class="s0">sum_wt2 += (new_wt * new_wt)</span>
                            <span class="s0">old_wt += new_wt</span>
                            <span class="s0">if not adjust:</span>
                                <span class="s0">sum_wt /= old_wt</span>
                                <span class="s0">sum_wt2 /= (old_wt * old_wt)</span>
                                <span class="s0">old_wt = 1.</span>
                <span class="s0">elif is_observation:</span>
                    <span class="s0">mean_x = cur_x</span>
                    <span class="s0">mean_y = cur_y</span>

                <span class="s0">if nobs &gt;= minp:</span>
                    <span class="s0">if not bias:</span>
                        <span class="s0">numerator = sum_wt * sum_wt</span>
                        <span class="s0">denominator = numerator - sum_wt2</span>
                        <span class="s0">if denominator &gt; 0:</span>
                            <span class="s0">sub_out[i] = (numerator / denominator) * cov</span>
                        <span class="s0">else:</span>
                            <span class="s0">sub_out[i] = NaN</span>
                    <span class="s0">else:</span>
                        <span class="s0">sub_out[i] = cov</span>
                <span class="s0">else:</span>
                    <span class="s0">sub_out[i] = NaN</span>

        <span class="s0">output[s:e] = sub_out</span>

    <span class="s0">return output</span>
</pre>
</body>
</html>